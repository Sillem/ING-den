{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data transformation for ING den ~ \"Neuralna ekipa\"\n",
    "\n",
    "This notebook describes pipeline construction. Some procedures are based on *analysis.ibynb* notebook, while others are new and their purpose will be described in documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import make_column_selector\n",
    "from feature_engine.encoding import WoEEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('https://files.challengerocket.com/files/lions-den-ing-2024/development_sample.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Division of variables, according to features types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "discrete_variables = ['ID', 'customer_id', 'Var1', 'Var15', 'Var16', 'Var20', 'Var21', 'Var22',\n",
    "                      \t'Var23', 'Var29', 'Var4', 'Var5', 'Var9', 'Var24', 'Var30', 'Var6'\n",
    "]\n",
    "\n",
    "continuous_variables = [\n",
    "    'Var7', 'Var8', 'Var10', \n",
    "    'Var17', 'Var25', 'Var26', '_r_'\n",
    "]\n",
    "\n",
    "binary_variables = [\n",
    "    'target', 'Application_status', 'Var18', \n",
    "    'Var19', 'Var27', 'Var28'\n",
    "]\n",
    "\n",
    "categorical_nominal_variables = [\n",
    "    'Var2', 'Var3', 'Var11', 'Var12', 'Var14'\n",
    "]\n",
    "\n",
    "\n",
    "datetime_variables = [\n",
    "    'application_date', 'Var13'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables not assigned yet: ALL ASSIGNED\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "assigned_vars = pd.Index(chain.from_iterable([discrete_variables, continuous_variables, \n",
    "binary_variables, categorical_nominal_variables, datetime_variables]))\n",
    "print(\"Variables not assigned yet:\", train_data.columns.difference(assigned_vars) if train_data.columns.difference(assigned_vars).shape[0] else \"ALL ASSIGNED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_xlsx = pd.read_excel('./variables_description.xlsx')\n",
    "#Słownik zmian nazw kolumn\n",
    "names = {f\"{names_xlsx['Column'][i]}\":f\"{names_xlsx['Description'][i]}\" for i in range(5, len(names_xlsx))}\n",
    "\n",
    "def rename_list(lista):\n",
    "    for idx in range(len(lista)):\n",
    "        if lista[idx] in names.keys():\n",
    "            lista[idx] = names[lista[idx]]\n",
    "    return lista\n",
    "\n",
    "discrete_variables = rename_list(discrete_variables)\n",
    "continuous_variables = rename_list(continuous_variables)\n",
    "binary_variables = rename_list(binary_variables)\n",
    "categorical_nominal_variables = rename_list(categorical_nominal_variables)\n",
    "datetime_variables = rename_list(datetime_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pre-Pipeline preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step involves steps removing some observations, so it is a step before the pipeline. In code below we create two regexes, based on which we will select variables for numerical/categorical processing. It simply automates the process of selecting variables for a given arm of ColumnTransformer (instead of passing long list of names that can change during the pipeline, we are interested only in the end of the name, so original one). Added variables is additional list of variables that will be added to the dataset, but here it doesn't bother us that they will be supplied in the regex (it is simply an alternative regex)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_regex():\n",
    "\n",
    "    added_variables = []\n",
    "    added_variables.append('durationOfEmployment')\n",
    "    added_variables.append('installmentPerIncomeOfMainApplicant')\n",
    "    added_variables.append('installmentPerIncome')\n",
    "    added_variables.append('incomeOfMainApplicantperChildrenNumber')\n",
    "    added_variables.append('incomeOfMainApplicantperdependencesNumber')\n",
    "    added_variables.append('installmentAmountPerIncomeAndGoods')\n",
    "    added_variables.append('installmentPerBothIncomes')\n",
    "    added_variables.append('dependentNumberOfChildrenOnRelationshipStatus')\n",
    "    \n",
    "    num_regex = \"^(.*)(\"\n",
    "    nominal_regex = \"^(.*)(\"\n",
    "    for num_feature in discrete_variables + continuous_variables + added_variables:\n",
    "        num_feature = num_feature.replace(')', '\\)').replace('(', '\\(')\n",
    "        num_regex+=num_feature+'|'\n",
    "    num_regex=num_regex[:-1] # removing last |\n",
    "    num_regex+=')$'\n",
    "\n",
    "    #lets build nominal feature regex selector\n",
    "    for cat_feature in categorical_nominal_variables:\n",
    "            nominal_regex+=cat_feature.replace(')', '\\)').replace('(', '\\(')+'|'\n",
    "    nominal_regex=nominal_regex[:-1] # removing last |\n",
    "    nominal_regex+=')$'\n",
    "    return num_regex, nominal_regex\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function does several different things:\n",
    "1. Renames columns according to mapping from official competition file\n",
    "2. Sets the index to one from dataset\n",
    "3. Removes observations based on supplied to the function list of variables that we won't impute in the future (described why in our earlier work)\n",
    "4. Removes rows that come with absurd date of 31Dec9999\n",
    "5. It returns preprocessed data that are splitted into training data, training labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_nans(X : pd.DataFrame, columns=['target', 'Spendings estimation']) -> pd.DataFrame:\n",
    "    \"\"\"Funkcja do wywalania wierszy które mają NaN w którejś z kolumn podanych w liście.\n",
    "    \n",
    "\n",
    "    Args:\n",
    "        X (pd.DataFrame): dataframe do przetworzenia (usunięcia wierszy). Ten surowy z URLa.\n",
    "        columns (list, optional): Kolumny z oryginalnego df (opisowe, nie VarX). \n",
    "        Z których wiersze z NaNami.\n",
    "        Defaults to ['target', 'Spendings estimation'].\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame pd.Series: Dataframe z danymi treningowymi, dataframe z labelkami\n",
    "    \"\"\"\n",
    "    X = X.rename(columns=names)\n",
    "    X = X.set_index('ID')\n",
    "    for column in columns:\n",
    "        X = X[X[column].notna()]\n",
    "    X = X[X['Application data: employment date (main applicant)'] != '31Dec9999']\n",
    "\n",
    "\n",
    "    \n",
    "    return X.drop(['target'], axis=1), X['target']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For testing purposes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_X, train_data_y = remove_nans(train_data)\n",
    "num_regex, nominal_regex = generate_regex()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixing encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some variables ('Distribtuon channel', 'Application_status') come with some wrong (but easy to fix) encodings. This function however doesn't affect the row count so it will be used in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_encodings(X : pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Tutaj sztywno zmieniam zepsute encodingi w danych kolumnach\n",
    "\n",
    "    Args:\n",
    "        X (pd.DataFrame): dataframe po użyciu remove_nans\n",
    "        with_FE (bool) : flaga na True jeżeli do danych dodajemy przetworzone zmienne\n",
    "    Returns:\n",
    "        pd.DataFrame: dataframe z poprawionymi encodingami\n",
    "    \"\"\"\n",
    "    X_copy = X.copy()\n",
    "    if 'Distribution channel' in X.columns:\n",
    "        X_copy['Distribution channel'] = X_copy['Distribution channel'].replace(\"Direct\", 1)\n",
    "        X_copy['Distribution channel'] = X_copy['Distribution channel'].replace(\"Broker\", 2)    \n",
    "        X_copy['Distribution channel'] = X_copy['Distribution channel'].replace(\"Online\", 3)\n",
    "\n",
    "    if 'Application_status' in X.columns:\n",
    "        X_copy['Application_status'] = X_copy['Application_status'].replace(\"Approved\", 1)\n",
    "        X_copy['Application_status'] = X_copy['Application_status'].replace(\"Rejected\", 0)\n",
    "        \n",
    "    return X_copy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engingeering\n",
    "Before processing the data, we must create some features that might be a significant help for our model. We must create them here, so they can be processed by pipeline (and they will be taken into consideration by pipeline, because of regexes we built before). Here are some economically economically substantive feature proposals:\n",
    "\n",
    "1. How long had have been person employed before loan application (application date - employment date)?\n",
    "2. Proportion of installment amount to income of main applicant\n",
    "3. Proportion of installment amount to average income\n",
    "4. Proportion of installment amount to amount on current account + amount on savings amount -- not possible due to NaNs in amount on current account\n",
    "5. Income of main applicant / number of children + 1 (the applicant)\n",
    "6. Income of main applicant / number of dependences + 1 (the applicant)\n",
    "7. Installment amount / average income + value of the goods\n",
    "8. Application amount / (value of the goods + amoutn on current account + amount of savings account) -- not possible due to NaNs\n",
    "9. Installment amount / income of main applicant + income of the second applicant\n",
    "10. Number of children / 2 if married/informal relationship number of children /1 otherwise\n",
    "11. Amount on savings account / amount on current account -- also not possible due to NaNs\n",
    "12. Bureau score > 0?\n",
    "\n",
    "Funtion below will prepare all variables in one step. As it doesn't change the dependent variable and row count, it will be a first step into the transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_new_features(X : pd.DataFrame) -> pd.DataFrame:\n",
    "    X_new = X.copy()\n",
    "    # durationOfEmployment\n",
    "    X_new['durationOfEmployment'] = pd.to_datetime(X_new['application_date']) - pd.to_datetime(X_new['Application data: employment date (main applicant)'], format=\"%d%b%Y\")\n",
    "    \n",
    "    # installment per average income of main applicant\n",
    "    X_new['installmentPerIncomeOfMainApplicant'] = X_new['Installment amount'] / X_new['Application data: income of main applicant']\n",
    "    \n",
    "    # installment amount per average income\n",
    "    X_new['installmentPerIncome'] = X_new['Installment amount'] / X_new['Average income (Exterval data)']\n",
    "    \n",
    "    # income of main applicant / number of children + 1\n",
    "    X_new['incomeOfMainApplicantperChildrenNumber'] = X_new['Application data: income of main applicant']/(X_new['Application data: number of children of main applicant'] + 1)\n",
    "\n",
    "    # income of main applicant / number of dependences + 1 (the applicant)\n",
    "    X_new['incomeOfMainApplicantperdependencesNumber'] = X_new['Application data: income of main applicant']/(X_new['Application data: number of dependences of main applicant'] + 1)\n",
    "    \n",
    "    # installment amount / average income + value of the goods\n",
    "    X_new['installmentAmountPerIncomeAndGoods'] = X_new['Installment amount']/(X_new['Average income (Exterval data)'] + X_new['Value of the goods (car)'].apply(lambda x: 0 if pd.isna(x) else x))\n",
    "    \n",
    "    # installment amount / income of main applicant + income of the second applicant\n",
    "    X_new['installmentPerBothIncomes'] = X_new['Installment amount'] / (X_new['Application data: income of main applicant'] + X_new['Application data: income of second applicant'].apply(lambda x: 0 if pd.isna(x) else x))\n",
    "    \n",
    "    # number of children per different options\n",
    "    X_new['dependentNumberOfChildrenOnRelationshipStatus'] = X_new['Application data: number of children of main applicant'].apply(lambda x: 0 if pd.isna(x) else x) / X_new['Application data: marital status of main applicant'].apply(lambda x: 2 if x in [1, 2] else 1)\n",
    "    \n",
    "    # bureau score > 0? this is done because 1st quartile of this variable is 10, and median is 0 so it is quite unique\n",
    "    X_new['isPositiveBureauScore'] = (X_new['Credit bureau score (Exterval data)'] > 0).astype('int64')\n",
    "    \n",
    "    \n",
    "    return X_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we wrap the function into FunctionTransformer for easy implementation into the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_features_transformer = FunctionTransformer(create_new_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we manually splitted* the variables that need individual treatment when it comes to imputing. The method is describet in first chapter of documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_for_zero_impute = ['Application data: income of second applicant', 'Application data: profession of second applicant', 'Value of the goods (car)']\n",
    "vars_for_add_category_impute = ['Property ownership for property renovation', 'Clasification of the vehicle (Car, Motorbike)']\n",
    "vars_for_mode_impute = ['Loan purpose', 'Distribution channel']\n",
    "vars_for_fill_zeros_but_add_var = [\"Amount on current account\", \"Amount on savings account\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some variables besides filling with 0 require marking the whole observation with new variable (to mark that for example there was no savings account) to differenciate empty accounts from non existing accounts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleImputeAddFeature(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns \n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        \n",
    "        for column in self.columns:\n",
    "            X_copy[column + '_was_missing'] = X_copy[column].isnull().astype(int)\n",
    "            \n",
    "            X_copy[column] = X_copy[column].fillna(0)\n",
    "        \n",
    "        return X_copy\n",
    "    \n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "       if input_features is None:\n",
    "           input_features = self.columns\n",
    "       output_features = np.concatenate([input_features, [f\"{col}_was_missing\" for col in self.columns]])\n",
    "       return output_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1st step of pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step we created various imputer for different subsets of variables (listed in *). Besides these basic imputations we use custom imputer written above and we fix encoding on Application Status separately. The rest of variables stay unchanged. Thanks to architecture of fix_encodings, this function, wrapped in FunctionTransformer generally (without error)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_imputer = SimpleImputer(strategy=\"constant\", fill_value=0)\n",
    "add_category_imputer = SimpleImputer(strategy=\"constant\", fill_value=2)\n",
    "mode_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "\n",
    "impute_column_transformer = ColumnTransformer([\n",
    "    (\"zero_fill\", zero_imputer, vars_for_zero_impute),\n",
    "    (\"add_third_category\", add_category_imputer, vars_for_add_category_impute),\n",
    "    (\"mode_impute\", make_pipeline(FunctionTransformer(fix_encodings), mode_imputer), vars_for_mode_impute),\n",
    "    (\"fill_zeros_but_add_var\", SimpleImputeAddFeature(vars_for_fill_zeros_but_add_var), vars_for_fill_zeros_but_add_var),\n",
    "    (\"application_status_transform\", FunctionTransformer(fix_encodings), ['Application_status'])\n",
    "    ],\n",
    "    remainder=\"passthrough\"\n",
    ").set_output(transform='pandas')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2nd step of pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for some reason the whole DataFrame is object type, and this won't work well with models so where we can (everything is cleanly encoded with an exception of datetime variables) we cast type to numeric. Then we create the instance of FunctionTransformer wrapping the created function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataframe_numeric_again(X : pd.DataFrame) -> pd.DataFrame:\n",
    "    X_copy = X.copy()\n",
    "    for column in X:\n",
    "        if column.split('__')[1] not in datetime_variables: \n",
    "            X_copy[column] = pd.to_numeric(X[column])\n",
    "    return X_copy\n",
    "\n",
    "numericTransformer = FunctionTransformer(make_dataframe_numeric_again)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3rd step of pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I want to scale and OneHotEncode variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_transform_transformer = ColumnTransformer([\n",
    "    (\"scale\", StandardScaler(), make_column_selector(num_regex)),\n",
    "    (\"one_hot_encode\", OneHotEncoder(sparse_output=False), make_column_selector(nominal_regex))\n",
    "],\n",
    "    remainder=\"passthrough\").set_output(transform=\"pandas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "feature_transform_transformer_woe = ColumnTransformer([\n",
    "    (\"scale\", StandardScaler(), make_column_selector(num_regex)),\n",
    "    (\"woe_encode\", WoEEncoder(ignore_format=True), make_column_selector(nominal_regex))\n",
    "],\n",
    "    remainder=\"passthrough\").set_output(transform=\"pandas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4rd step of pipeline\n",
    "Now I will remove varialbes that wont be usable anymore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unnecesary(X : pd.DataFrame) -> pd.DataFrame:\n",
    "    return X.drop(['remainder__remainder__Application data: employment date (main applicant)',\n",
    "                   'remainder__remainder__application_date',\n",
    "                   'remainder__application_status_transform__Application_status',\n",
    "                   'scale__remainder__customer_id'\n",
    "                  ], axis=1)\n",
    "\n",
    "remove_unnecesary_transformer = FunctionTransformer(remove_unnecesary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sillem/anaconda3/lib/python3.11/site-packages/sklearn/compose/_column_transformer.py:1123: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  cols = cols[cols.str.contains(self.pattern, regex=True)]\n"
     ]
    }
   ],
   "source": [
    "full = make_pipeline(create_features_transformer, impute_column_transformer, numericTransformer, feature_transform_transformer_woe, remove_unnecesary_transformer)\n",
    "data = full.fit_transform(train_data_X, train_data_y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
