{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data transformation for ING den\n",
    "## Neuralna ekipa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import make_column_selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('https://files.challengerocket.com/files/lions-den-ing-2024/development_sample.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Division of variables, according to features types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "discrete_variables = ['ID', 'customer_id', 'Var1', 'Var15', 'Var16', 'Var20', 'Var21', 'Var22',\n",
    "                      \t'Var23', 'Var29', 'Var4', 'Var5', 'Var9', 'Var24', 'Var30', 'Var6'\n",
    "]\n",
    "\n",
    "continuous_variables = [\n",
    "    'Var7', 'Var8', 'Var10', \n",
    "    'Var17', 'Var25', 'Var26', '_r_'\n",
    "]\n",
    "\n",
    "binary_variables = [\n",
    "    'target', 'Application_status', 'Var18', \n",
    "    'Var19', 'Var27', 'Var28'\n",
    "]\n",
    "\n",
    "categorical_nominal_variables = [\n",
    "    'Var2', 'Var3', 'Var11', 'Var12', 'Var14'\n",
    "]\n",
    "\n",
    "\n",
    "datetime_variables = [\n",
    "    'application_date', 'Var13'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables not assigned yet: ALL ASSIGNED\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "assigned_vars = pd.Index(chain.from_iterable([discrete_variables, continuous_variables, \n",
    "binary_variables, categorical_nominal_variables, datetime_variables]))\n",
    "print(\"Variables not assigned yet:\", train_data.columns.difference(assigned_vars) if train_data.columns.difference(assigned_vars).shape[0] else \"ALL ASSIGNED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_xlsx = pd.read_excel('./variables_description.xlsx')\n",
    "#Słownik zmian nazw kolumn\n",
    "names = {f\"{names_xlsx['Column'][i]}\":f\"{names_xlsx['Description'][i]}\" for i in range(5, len(names_xlsx))}\n",
    "\n",
    "def rename_list(lista):\n",
    "    for idx in range(len(lista)):\n",
    "        if lista[idx] in names.keys():\n",
    "            lista[idx] = names[lista[idx]]\n",
    "    return lista\n",
    "\n",
    "discrete_variables = rename_list(discrete_variables)\n",
    "continuous_variables = rename_list(continuous_variables)\n",
    "binary_variables = rename_list(binary_variables)\n",
    "categorical_nominal_variables = rename_list(categorical_nominal_variables)\n",
    "datetime_variables = rename_list(datetime_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usuwanie NaNów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_nans(X : pd.DataFrame, columns=['target', 'Spendings estimation']) -> pd.DataFrame:\n",
    "    \"\"\"Funkcja do wywalania wierszy które mają NaN w którejś z kolumn podanych w liście.\n",
    "    \n",
    "\n",
    "    Args:\n",
    "        X (pd.DataFrame): dataframe do przetworzenia (usunięcia wierszy). Ten surowy z URLa.\n",
    "        columns (list, optional): Kolumny z oryginalnego df (opisowe, nie VarX). \n",
    "        Z których wiersze z NaNami.\n",
    "        Defaults to ['target', 'Spendings estimation'].\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Dataframe z nazwami opisowymi\n",
    "    \"\"\"\n",
    "    X = X.rename(columns=names)\n",
    "    for column in columns:\n",
    "        X = X[X[column].notna()]\n",
    "    return X\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does it work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(remove_nans(train_data)['target'].isna().any())\n",
    "print(remove_nans(train_data)['Spendings estimation'].isna().any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixing encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_encodings(X : pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Tutaj sztywno zmieniam zepsute encodingi w danych kolumnach\n",
    "\n",
    "    Args:\n",
    "        X (pd.DataFrame): dataframe po użyciu remove_nans\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: dataframe z poprawionymi encodingami\n",
    "    \"\"\"\n",
    "    X_copy = X.copy()\n",
    "    if 'Distribution channel' in X.columns:\n",
    "        X_copy['Distribution channel'] = X_copy['Distribution channel'].replace(\"Direct\", 1)\n",
    "        X_copy['Distribution channel'] = X_copy['Distribution channel'].replace(\"Broker\", 2)    \n",
    "        X_copy['Distribution channel'] = X_copy['Distribution channel'].replace(\"Online\", 3)\n",
    "\n",
    "    if 'Application_status' in X.columns:\n",
    "        X_copy['Application_status'] = X_copy['Application_status'].replace(\"Approved\", 1)\n",
    "        X_copy['Application_status'] = X_copy['Application_status'].replace(\"Rejected\", 0)\n",
    "    return X_copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_for_zero_impute = ['Application data: income of second applicant', 'Application data: profession of second applicant', 'Value of the goods (car)']\n",
    "vars_for_add_category_impute = ['Property ownership for property renovation', 'Clasification of the vehicle (Car, Motorbike)']\n",
    "vars_for_mode_impute = ['Loan purpose', 'Distribution channel']\n",
    "vars_for_fill_zeros_but_add_var = [\"Amount on current account\", \"Amount on savings account\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleImputeAddFeature(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns # Lista kolumn do transformacji\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # W fit nic nie musimy robić, ale musi być obecna\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Tworzymy kopię, aby nie modyfikować oryginalnego DataFrame\n",
    "        X_copy = X.copy()\n",
    "        \n",
    "        for column in self.columns:\n",
    "            # Dodajemy nową kolumnę z wartościami 0 i 1\n",
    "            X_copy[column + '_was_missing'] = X_copy[column].isnull().astype(int)\n",
    "            \n",
    "            # Simple impute - zamieniamy NaN na 0\n",
    "            X_copy[column] = X_copy[column].fillna(0)\n",
    "        \n",
    "        return X_copy\n",
    "    \n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "       if input_features is None:\n",
    "           input_features = self.columns\n",
    "       # Zakładając, że self.columns zawiera cechy, które zostały przetworzone\n",
    "       output_features = np.concatenate([input_features, [f\"{col}_was_missing\" for col in self.columns]])\n",
    "       return output_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1st step of pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impute the NaNs with methods explained during Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_imputer = SimpleImputer(strategy=\"constant\", fill_value=0)\n",
    "add_category_imputer = SimpleImputer(strategy=\"constant\", fill_value=2)\n",
    "mode_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "\n",
    "impute_column_transformer = ColumnTransformer([\n",
    "    (\"zero_fill\", zero_imputer, vars_for_zero_impute),\n",
    "    (\"add_third_category\", add_category_imputer, vars_for_add_category_impute),\n",
    "    (\"mode_impute\", make_pipeline(FunctionTransformer(fix_encodings), mode_imputer), vars_for_mode_impute),\n",
    "    (\"fill_zeros_but_add_var\", SimpleImputeAddFeature(vars_for_fill_zeros_but_add_var), vars_for_fill_zeros_but_add_var),\n",
    "    (\"application_status_transform\", FunctionTransformer(fix_encodings), ['Application_status'])\n",
    "    ],\n",
    "    remainder=\"passthrough\"\n",
    ").set_output(transform='pandas')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2nd step of pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to make whole dataframe numeric when I can."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataframe_numeric_again(X : pd.DataFrame) -> pd.DataFrame:\n",
    "    X_copy = X.copy()\n",
    "    for column in X:\n",
    "        if column.split('__')[1] not in datetime_variables: \n",
    "            X_copy[column] = pd.to_numeric(X[column])\n",
    "    return X_copy\n",
    "\n",
    "numericTransformer = FunctionTransformer(make_dataframe_numeric_again)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3rd step of pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I want to scale and OneHotEncode variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^(.*)(ID|customer_id|Number of applicants|Application data: number of children of main applicant|Application data: number of dependences of main applicant|Number of requests during the last 3 months \\(External data\\)|Number of requests during the last 6 months \\(External data\\)|Number of requests during the last 9 months \\(External data\\)|Number of requests during the last 12 months \\(External data\\)|Credit bureau score \\(Exterval data\\)|Application amount|Credit duration \\(months\\)|Application data: income of main applicant|Limit on credit card|Average income \\(Exterval data\\)|Payment frequency|Installment amount|Value of the goods \\(car\\)|Application data: income of second applicant|Spendings estimation|Amount on current account|Amount on savings account|_r_)$\n"
     ]
    }
   ],
   "source": [
    "num_regex = \"^(.*)(\"\n",
    "for num_feature in discrete_variables + continuous_variables:\n",
    "    num_feature = num_feature.replace(')', '\\)').replace('(', '\\(')\n",
    "    num_regex+=num_feature+'|'\n",
    "num_regex=num_regex[:-1] # removing last |\n",
    "num_regex+=')$'\n",
    "print(num_regex)\n",
    "#lets build nominal feature regex selector\n",
    "nominal_regex = \"^(.*)(\"\n",
    "for cat_feature in categorical_nominal_variables:\n",
    "        nominal_regex+=cat_feature+'|'\n",
    "nominal_regex=nominal_regex[:-1]\n",
    "nominal_regex+=')$'\n",
    "\n",
    "feature_transform_transformer = ColumnTransformer([\n",
    "    (\"scale\", StandardScaler(), make_column_selector(num_regex)),\n",
    "    (\"one_hot_encode\", OneHotEncoder(sparse_output=False), make_column_selector(nominal_regex))\n",
    "],\n",
    "    remainder=\"passthrough\").set_output(transform=\"pandas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_transform_transformer_woe = ColumnTransformer([\n",
    "    (\"scale\", StandardScaler(), make_column_selector(num_regex)),\n",
    "    (\"woe_encode\", WoEEncoder(ignore_format=True), make_column_selector(nominal_regex))\n",
    "],\n",
    "    remainder=\"passthrough\").set_output(transform=\"pandas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7710/3345071611.py:17: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  X_copy['Application_status'] = X_copy['Application_status'].replace(\"Approved\", 1)\n",
      "/home/sillem/anaconda3/envs/ING_den/lib/python3.10/site-packages/sklearn/compose/_column_transformer.py:1462: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  cols = cols[cols.str.contains(self.pattern, regex=True)]\n"
     ]
    }
   ],
   "source": [
    "#full = make_pipeline(impute_column_transformer, numericTransformer, feature_transform_transformer)\n",
    "train_data_no_nans = remove_nans(train_data)\n",
    "train_data_y = train_data_no_nans['target']\n",
    "train_data_X = train_data_no_nans.drop(['target'], axis=1)\n",
    "full = make_pipeline(impute_column_transformer, numericTransformer, feature_transform_transformer_woe)\n",
    "data = full.fit_transform(train_data_processed, train_data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_engine.encoding import WoEEncoder, RareLabelEncoder"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ING_den",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
